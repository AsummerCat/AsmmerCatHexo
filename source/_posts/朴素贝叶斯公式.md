---
title: 朴素贝叶斯公式
date: 2020-01-20 15:29:04
tags: [数学]
---

# 朴素贝叶斯公式

  朴素贝叶斯中的朴素一词的来源就是假设各特征之间相互独立。这一假设使得朴素贝叶斯算法变得简单，但有时会牺牲一定的分类准确率。

首先给出贝叶斯公式：![贝叶斯公式](/img/2020-01-20/1.png)

换成分类任务的表达式：![分类任务的表达式](/img/2020-01-20/2.png)

 我们最终求的p(类别|特征)即可！就相当于完成了我们的任务。
 则，朴素贝特斯公式为

![朴素贝特斯公式](/img/2020-01-20/3.png)

<!--more-->

## 实例解析

首先，给出数据如下:

![数据集](/img/2020-01-20/4.png)

现在给我们的问题是，如果一对男女朋友，男生想女生求婚，男生的四个特点分别是不帅，性格不好，身高矮，不上进，请你判断一下女生是嫁还是不嫁？

这是典型的二分类问题，按照朴素贝叶斯的求解，转换为P(嫁|不帅、性格不好、矮、不上进)和P(不嫁|不帅、性格不好、矮、不上进)的概率，最终选择嫁与不嫁的答案。

这里我们根据贝特斯公式:

![根据贝特斯公式](/img/2020-01-20/5.png)

由此，我们将(嫁|不帅、性格不好、矮、不上进)转换成三个可求的P(嫁)、P(不帅、性格不好、矮、不上进|嫁)、P(不帅、性格不好、矮、不上进)。进一步分解可以得:

```python
P(不帅、性格不好、矮、不上进)=P(嫁)P(不帅|嫁)P(性格不好|嫁)P(矮|嫁)P(不上进|嫁)+P(不嫁)P(不帅|不嫁)P(性格不好|不嫁)P(矮|不嫁)P(不上进|不嫁)。
P(不帅、性格不好、矮、不上进|嫁)=P(不帅|嫁)P(性格不好|嫁)P(矮|嫁)P(不上进|嫁)
```

将上面的公式整理一下可得:
![公式整理](/img/2020-01-20/6.png)

P(嫁)=1/2、P(不帅|嫁)=1/2、P(性格不好|嫁)=1/6、P(矮|嫁)=1/6、P(不上进|嫁)=1/6。
 P(不嫁)=1/2、P(不帅|不嫁)=1/3、P(性格不好|不嫁)=1/2、P(矮|不嫁)=1、P(不上进|不嫁)=2/3
 但是由贝叶斯公式可得:对于目标求解为不同的类别，贝叶斯公式的分母总是相同的。所以，只求解分子即可：
![公式整理](/img/2020-01-20/7.png)

```python
于是，对于类别“嫁”的贝叶斯分子为：P(嫁)P(不帅|嫁)P(性格不好|嫁)P(矮|嫁)P(不上进|嫁)=1/2 * 1/2 * 1/6 * 1/6 * 1/6=1/864     
对于类别“不嫁”的贝叶斯分子为:P(不嫁)P(不帅|不嫁)P(性格不好|不嫁)P(矮|不嫁)P(不上进|不嫁)=1/2 * 1/3 * 1/2 * 1* 2/3=1/18。
经代入贝叶斯公式可得：P(嫁|不帅、性格不好、矮、不上进)=(1/864) / (1/864+1/18)=1/49=2.04%
P(不嫁|不帅、性格不好、矮、不上进)=(1/18) / (1/864+1/18)=48/49=97.96%
则P(不嫁|不帅、性格不好、矮、不上进) > P(嫁|不帅、性格不好、矮、不上进)，则该女子选择不嫁！
```

## 朴素贝叶斯的优缺点

```python
优点：
  (1） 算法逻辑简单,易于实现（算法思路很简单，只要使用贝叶斯公式转化即可！）
（2）分类过程中时空开销小（假设特征相互独立，只会涉及到二维存储）
缺点：
      朴素贝叶斯假设属性之间相互独立，这种假设在实际过程中往往是不成立的。在属性之间相关性越大，分类误差也就越大。
```

#  朴素贝叶斯实战

sklearn中有3种不同类型的朴素贝叶斯：

* 高斯分布型：用于classification问题，假定属性/特征服从正态分布的。

````
高斯朴素贝叶斯 

GaussianNB 实现了运用于分类的高斯朴素贝叶斯算法。特征的可能性(即概率)假设为高斯分布:
````



* 多项式型：用于离散值模型里。比如文本分类问题里面我们提到过，我们不光看词语是否在文本中出现，也得看出现次数。如果总词数为n，出现词数为m的话，有点像掷骰子n次出现m次这个词的场景。

```
MultinomialNB 实现了服从多项分布数据的朴素贝叶斯算法，也是用于文本分类(这个领域中数据往往以词向量表示，尽管在实践中 tf-idf 向量在预测时表现良好)的两大经典朴素贝叶斯算法之一。
```



* 伯努利型：最后得到的特征只有0(没出现)和1(出现过)。

```
 BernoulliNB 实现了用于多重伯努利分布数据的朴素贝叶斯训练和分类算法，即有多个特征，但每个特征 都假设是一个二元 (Bernoulli, boolean) 变量。 因此，这类算法要求样本以二元值特征向量表示；如果样本含有其他类型的数据， 一个 BernoulliNB 实例会将其二值化(取决于 binarize 参数)。
```



## 我们使用iris数据集进行分类

```python

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score
from sklearn import datasets
iris = datasets.load_iris()
gnb = GaussianNB()
scores=cross_val_score(gnb, iris.data, iris.target, cv=10)
print("Accuracy:%.3f"%scores.mean())
输出: Accuracy:0.953
```



## 旧金山犯罪分类预测

  题目背景：『水深火热』的大米国，在旧金山这个地方，一度犯罪率还挺高的，然后很多人都经历过大到暴力案件，小到东西被偷，车被划的事情。当地警方也是努力地去总结和想办法降低犯罪率，一个挑战是在给出犯罪的地点和时间的之后，要第一时间确定这可能是一个什么样的犯罪类型，以确定警力等等。后来干脆一不做二不休，直接把12年内旧金山城内的犯罪报告都丢带Kaggle上，说『大家折腾折腾吧，看看谁能帮忙第一时间预测一下犯罪类型』。犯罪报告里面包括日期，描述，星期几，所属警区，处理结果，地址，GPS定位等信息。当然，分类问题有很多分类器可以选择，我们既然刚讲过朴素贝叶斯，刚好就拿来练练手好了。

### **首先我们来看一下数据**

```python
import pandas as pd  
import numpy as np  
from sklearn import preprocessing  
from sklearn.metrics import log_loss  
from sklearn.cross_validation import train_test_split
train = pd.read_csv('/Users/liuming/projects/Python/ML数据/Kaggle旧金山犯罪类型分类/train.csv', parse_dates = ['Dates'])  
test = pd.read_csv('/Users/liuming/projects/Python/ML数据/Kaggle旧金山犯罪类型分类/test.csv', parse_dates = ['Dates'])  
train 

```

![旧金山犯罪分类预测](/img/2020-01-20/8.png)

```python
我们依次解释一下每一列的含义：

Date: 日期
Category: 犯罪类型，比如 Larceny/盗窃罪 等.
Descript: 对于犯罪更详细的描述
DayOfWeek: 星期几
PdDistrict: 所属警区
Resolution: 处理结果，比如说『逮捕』『逃了』
Address: 发生街区位置
X and Y: GPS坐标
        train.csv中的数据时间跨度为12年，包含了将近90w的记录。另外，这部分数据，大家从上图上也可以看出来，大部分都是『类别』型，比如犯罪类型，比如星期几。

```

### 特征预处理

sklearn.preprocessing模块中的[ LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)函数可以对类别做编号，我们用它对犯罪类型做编号；pandas中的[get_dummies( )](http://pandas.pydata.org/pandas-docs/version/0.13.1/generated/pandas.get_dummies.html)可以将变量进行二值化01向量，我们用它对”街区“、”星期几“、”时间点“进行因子化。

```python
#对犯罪类别:Category; 用LabelEncoder进行编号  
leCrime = preprocessing.LabelEncoder()  
crime = leCrime.fit_transform(train.Category)   #39种犯罪类型  
#用get_dummies因子化星期几、街区、小时等特征  
days=pd.get_dummies(train.DayOfWeek)  
district = pd.get_dummies(train.PdDistrict)  
hour = train.Dates.dt.hour  
hour = pd.get_dummies(hour)  
#组合特征  
trainData = pd.concat([hour, days, district], axis = 1)  #将特征进行横向组合  
trainData['crime'] = crime   #追加'crime'列  
days = pd.get_dummies(test.DayOfWeek)  
district = pd.get_dummies(test.PdDistrict)  
hour = test.Dates.dt.hour  
hour = pd.get_dummies(hour)  
testData = pd.concat([hour, days, district], axis=1)  
trainData 
```

特征预处理后，训练集feature，如下图所示：

![数据预处理](/img/2020-01-20/9.png)

###  建模

```python
from sklearn.naive_bayes import BernoulliNB
import time
features=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday', 'BAYVIEW', 'CENTRAL', 'INGLESIDE', 'MISSION',  
 'NORTHERN', 'PARK', 'RICHMOND', 'SOUTHERN', 'TARAVAL', 'TENDERLOIN']  
X_train, X_test, y_train, y_test = train_test_split(trainData[features], trainData['crime'], train_size=0.6)  
NB = BernoulliNB()  
nbStart = time.time()  
NB.fit(X_train, y_train)  
nbCostTime = time.time() - nbStart  
#print(X_test.shape)  
propa = NB.predict_proba(X_test)   #X_test为263415*17； 那么该行就是将263415分到39种犯罪类型中，每个样本被分到每一种的概率  
print("朴素贝叶斯建模%.2f秒"%(nbCostTime))  
predicted = np.array(propa)  
logLoss=log_loss(y_test, predicted)  
print("朴素贝叶斯的log损失为:%.6f"%logLoss)  
```

### 输出

```python
朴素贝叶斯建模0.55秒
朴素贝叶斯的log损失为:2.582561
```



## 分词参考

https://blog.csdn.net/weixin_42398658/article/details/82967039

### 标记

```python
#!/usr/bin/env/python
# -*- coding: utf-8 -*-
# Author: 赵守风
# File name: bayes.py
# Time:2018/9/28
# Email:1583769112@qq.com
from numpy import *
 
 
# 准备数据阶段，本阶段主要是把文本数据转换成词条向量，该词条背景来源斑点犬爱好者留言板，这些留言文本被切分成
# 一系列的词条集合，标点已经去掉
def load_data_set():
    posting_list = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
    class_vec = [0, 1, 0, 1, 0, 1]    # 类别标签，1是辱骂性文字，0是正常言论，标签数据是人工标签，用于训练模型
    return posting_list, class_vec
 
 
# 创建一个包含在所有文档中出现不重复的词的列表，这个词汇表可以根据现实情况给出，可以人为选择设置，性质和特征类似
def create_vocablist(dataset):
    vocabset = set([])  # 创建一个空集
    for document in dataset:
        vocabset = vocabset | set(document)  # 创建两个集合的并集
    return list(vocabset)     # 返回列表型的词汇数据
 
 
 
# 本模型为词集模型，每个词只能出现一次，又称基于贝努利模型的，其缺点是无法挖掘一个词出现多次的情况，因此引入词袋模型
# 又称基于多项式模型的分类器
# 判断文档有没有单词和建立的单词向量相同，并划分，输入的是词汇列表和某一个文档，输出的是文档向量
# 文档向量的每一个元素分别为1或者0，表示词汇表中的单词是否在文档中出现
def set_of_words2vec(vocablist, inputset):
    return_vec = [0]*len(vocablist)  # 创建一个向量，这个向量和输入的词汇向量长度一致
    for word in inputset:  # 遍历输入文档所有单词，并与词汇标准的单词比对是否出现
        if word in vocablist:  # 查看文档向量的单词是否在建立的词汇向量里
            return_vec[vocablist.index(word)] = 1  # 如果在词汇向量里，则该位置为1，反之为0,注意即使重复出现
                                                   # 也是不停的赋值为1，无法解决文本一个词多次出现的情况
        else:
            print('the word: %s is not in my vocablist!' % word)
    return return_vec
 
 
 
# 朴素贝叶斯词袋模型又称基于多项式模型
# 判断文档有没有单词和建立的单词向量相同，并划分，输入的是词汇列表和某一个文档，输出的是文档向量
# 文档向量的每一个元素分别为1或者0，表示词汇表中的单词是否在文档中出现
def bag_of_words2vec(vocablist, inputset):
    return_vec = [0]*len(vocablist)  # 创建一个向量，这个向量和输入的词汇向量长度一致
    for word in inputset:  # 遍历输入文档所有单词，并与词汇标准的单词比对是否出现
        if word in vocablist:  # 查看文档向量的单词是否在建立的词汇向量里
            return_vec[vocablist.index(word)] += 1  # 如果在词汇向量里，则该位置为1，反之为0
    return return_vec
 
 
# 朴素贝叶斯分类器训练函数
def train_nb0(train_matrix, train_category):  # train_matrix为输入样本矩阵，train_category为分类标签数据
    num_train_docs = len(train_matrix)  # 获得训练样本（文档）的个数
    num_words = len(train_matrix[0])  # 获得矩阵中每个样本（文档）的维度，每个文档多少词
    p_abusive = sum(train_category) / float(num_train_docs)  # 计算辱骂性文档出现的概率
    p0_num = ones(num_words)  # 为了避免概率为零的错误。初始化每个单词出现一次，并且分母初始化为2
    p1_num = ones(num_words)
    p0_denom = 2.0
    p1_denom = 2.0
    for i in range(num_train_docs):
        if train_category[i] == 1:  # 侮辱性文档
             p1_num += train_matrix[i]  # 出现的侮辱性的单词对应累加
             p1_denom += sum(train_matrix[i])  # 出现侮辱性的词总数叠加
        else:                          # 不是侮辱性文档
            p0_num += train_matrix[i]  # 出现的侮辱性的单词对应相加
            p0_denom += sum(train_matrix[i])  # 出现侮辱性的词总数叠加
 
    # 使用对数的目的是为了避免概率连乘很小溢出
    p1_vect = log(p1_num / p1_denom)  # 侮辱性文档中出现侮辱性词汇的概率
    p0_vect = log(p0_num / p1_denom)  # 正常文档中出现侮辱性词汇的概率
 
    return p0_vect, p1_vect, p_abusive
 
# 朴素贝叶斯计算
def classifynb(vec2classify, p0_vec, p1_vec, pclass1):
    # 解释一下下面的代码，下面的代码求得是条件概率即：p(ci/w) = {p(w/ci)*p(ci)}/p(w),其中ci为类别，w为特征向量
    # 意思就是根据样本数据求出在某一类别的情况下，出现w特征的概率即p(w/ci)，在乘上类别的概率p(ci)，
    # 在除以特征的概率p(w) 就可以得到具有某些特征属于某一类别的概率p(ci/w)
    # 例如vec2classify为文本数据出现的特征词汇的向量为[1,0,0,1,0,0,0,0,1....], p1vec为前面计算出来的
    # 侮辱性文档中出现侮辱性词汇的概率即p(w/ci)，如[0.0526 0.05263158 0.05263158 0.05263158 0.05263158 0.05263158...]，
    # 现在vec2classify * p1_vec就是特征单词出现的概率，再求sum是因为前面计算p1_vec是log，相加就是相乘，最后在+log(pclass1)
    # 也是相乘，本来还需要同时除以p(w)的，但是因为所有的计算都除，而且只是比较大小，因此可以都不除这个分母
    p1 = sum(vec2classify * p1_vec) + log(pclass1)
 
    p0 = sum(vec2classify * p0_vec) + log(1 - pclass1)
    if p1 > p0:
        return 1
    else:
        return 0
 
 
# 测试朴素贝叶斯分类器
def testingnb():
    listoposts, listclasses = load_data_set()   # 加载样本和标签数据
    myvocablist = create_vocablist(listoposts)  # 创建特征词汇列表
    train_mat = []  # 创建文本空矩阵
    for postindoc in listoposts:  # 遍历文本的单词和特征词汇列表比对，出现词汇列表的词时为1反之为0
        train_mat.append(set_of_words2vec(myvocablist, postindoc))
    p0v, p1v, pab = train_nb0(array(train_mat), array(listclasses))  # 计算先验概率，即通过样本计算p(w/ci)和p(ci)，也是训练权值
    testentry = ['love', 'my', 'dalmation']  # 测试数据
    thisdoc = array(set_of_words2vec(myvocablist, testentry))  # 测试样本数值化
    print(testentry, 'classifiled as；', classifynb(thisdoc, p0v, p1v, pab))  # 分类，并打印结果
    testentry = ['stupid', 'garbage']
    thisdoc = array(set_of_words2vec(myvocablist, testentry))
    print(testentry, 'classifiled as；', classifynb(thisdoc, p0v, p1v, pab))
```

下面给出调用上面的函数，进行邮件分类的试验，其中开始部分注释的内容 为测试某些python功能进行的试验：

```python
#!/usr/bin/env/python
# -*- coding: utf-8 -*-
# Author: 赵守风
# File name: ex_bayes_email_filter.py
# Time:2018/9/29
# Email:1583769112@qq.com
import re
import bayes
import numpy as np
 
'''
# 准备数据：切分文本
mysent = 'This book is the best book on python or M.L. I have ever laid eyes upon.'
print(mysent.split())  # 切割字符串，但是把标点符号也切割进去了
# ['This', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'M.L.', 'I', 'have', 'ever',
#  'laid', 'eyes', 'upon.']
# reg_ex = re.compile('\\W*')
# 分隔符是  除单词、数字外的任意字符串,得到去除标点的切分字符串
list_of_tokens = re.split('\\W*', mysent)
print(list_of_tokens)
# ['This', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'M', 'L', 'I', 'have', 'ever',
# 'laid', 'eyes', 'upon', '']
# 删除空格
temp = [tok for tok in list_of_tokens if len(tok) > 0]
print(temp)
# ['This', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'M', 'L', 'I', 'have', 'ever',
# 'laid', 'eyes', 'upon']
# 转换大写字母为小写字母
temp1 = [tok.lower() for tok in list_of_tokens if len(tok) > 0]
print(temp1)
# ['this', 'book', 'is', 'the', 'best', 'book', 'on', 'python', 'or', 'm', 'l', 'i', 'have', 'ever',
#  'laid', 'eyes', 'upon']
emailtext = open('email/ham/6.txt').read()
list_of_email = re.split('\\W*', emailtext)
print(list_of_email)
'''
 
 
# 文本解析，把文本读入，并切分成单个字符串，把大写字母转换成小写字母，只保留数字和字母，同时剔除小于3的字符串
def textparse(bigstring):
    import re
    list_of_tokens = re.split(r'\W*', bigstring)
    return [tok.lower() for tok in list_of_tokens if len(tok) > 2]
 
 
# 垃圾邮件测试
def spamtest():
    doclist = []
    classlist = []
    fulltext = []
    for i in range(1, 26):
        wordlist = textparse(open('email/spam/%d.txt' % i).read())
        doclist.append(wordlist)
        fulltext.extend(wordlist)
        classlist.append(1)
        wordlist = textparse(open('email/ham/%d.txt' % i).read())
        # 运行时会出错，错误提示为UnicodeDecodeError: 'gbk' codec can't decode byte 0xae in position 199:
        #  illegal multib，原因是在email/ham/%d.txt中的第23个文件出错，打开直接保存就好，不用修改任何内容
        doclist.append(wordlist)
        fulltext.extend(wordlist)
        classlist.append(0)
 
    vocablist = bayes.create_vocablist(doclist)
    trainingset = list(range(50))
    testset = []
 
    for i in range(10):
        randindex = int(np.random.uniform(0, len(trainingset)))
        testset.append(trainingset[randindex])
        del(trainingset[randindex])
        # 运行到这里会出错，这是因为Python3中range不在返回数值，修改办法为：
        # trainingset = range(50)改为trainingset = list(range(50))
 
    train_mat = []
    train_classes = []
    for docindex in trainingset:
        train_mat.append(bayes.set_of_words2vec(vocablist, doclist[docindex]))
        train_classes.append(classlist[docindex])
 
    p0v, p1v, pspam = bayes.train_nb0(np.array(train_mat), np.array(train_classes))
    errorcount = 0
    class_error = []
    for docindex in testset:
        word_vector = bayes.set_of_words2vec(vocablist, doclist[docindex])
        if bayes.classifynb(np.array(word_vector), p0v, p1v, pspam) != classlist[docindex]:
            errorcount += 1
            class_error.append(doclist[docindex])
    print('the error rate is : ', float(errorcount)/len(testset))
    print('错误分类的文本： ', class_error)
```



### 另外一部分案例

```python
from sklearn import datasets
from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
import pandas as pd
import numpy as np
 
breast_cancer = datasets.load_breast_cancer() # 乳腺癌数据
 
# sklearn自带的数据已经帮我们分好了，自带了很多属性，可以直接调用，但是建议能找到一份原始数据，
# 自己进行处理，然后在调用模型，后面有机会我会这样多做几个几个项目
 
# 高斯朴素贝叶斯
gnb = GaussianNB()
y_pred = gnb.fit(breast_cancer.data, breast_cancer.target).predict(breast_cancer.data)
print("Number of mislabeled points out of a total %d points : %d" % (breast_cancer.data.shape[0],(breast_cancer.target != y_pred).sum()))
print('正确率为：%f ' %(1 - ((breast_cancer.target != y_pred).sum()/breast_cancer.data.shape[0])))
 
输出：
Number of mislabeled points out of a total 569 points : 33
正确率为：0.942004 
 
 
 
# 多项分布朴素贝叶斯
mnb = MultinomialNB()
y_pred = mnb.fit(breast_cancer.data, breast_cancer.target).predict(breast_cancer.data)
print("Number of mislabeled points out of a total %d points : %d" % (breast_cancer.data.shape[0],(breast_cancer.target != y_pred).sum()))
print('正确率为：%f ' %(1 - ((breast_cancer.target != y_pred).sum()/breast_cancer.data.shape[0])))
 
 
输出：
Number of mislabeled points out of a total 569 points : 59
正确率为：0.896309 
 
 
# 伯努利朴素贝叶斯
bnb = BernoulliNB()
y_pred = bnb.fit(breast_cancer.data, breast_cancer.target).predict(breast_cancer.data)
print("Number of mislabeled points out of a total %d points : %d" % (breast_cancer.data.shape[0],(breast_cancer.target != y_pred).sum()))
print('正确率为：%f ' %(1 - ((breast_cancer.target != y_pred).sum()/breast_cancer.data.shape[0])))
 
Number of mislabeled points out of a total 569 points : 212
正确率为：0.627417
```

