---
title: 面试相关-分布式搜索
date: 2020-02-14 14:48:34
tags: [面试相关]
---

# es



## 最小存储单位

```
存取数据单位 :索引 

索引的概念:创建一个order_idex索引, 所有的订单数据都写入到这个索引中去  相当于(mysql一张表)
索引结构: index->type->mapping->document->field

index: mysql的一张表
type: 拥有多个数据结构  大部分字段相等 类似 多个表结构(虚拟)   (一般来就一个type)
mapping:  每个type有个mapping   mapping代表type的表结构定义 (每个字段名称 配置等)
document: type具体存放的内容
field:  每个field代表了这个document中的一个字段的值
```

<!--more-->

## lucene底层

```
倒排索引

举例: 
比如有三个地址:
url1 -> “我爱北京”
url2 -> “我爱到家”
url3 -> “到家美好”
这是一个正排索引Map<url, page_content>。

分词为 
url1 -> {我，爱，北京}
url2 -> {我，爱，到家}
url3 -> {到家，美好}
这是一个分词后的正排索引Map<url, list<item>>。

分词后倒排索引：
我 -> {url1, url2}
爱 -> {url1, url2}
北京 -> {url1}
到家 -> {url2, url3}
美好 -> {url3}
由检索词item快速找到包含这个查询词的网页Map<item, list<url>>就是倒排索引。

查询时间复杂度为O(1) 大大提升了效率


```



# 真题

## es和solr的区别

```
建立索引     solr阻塞       es不会阻塞
动态添加数据 solr会变慢      es没变化
分布式管理   solr利用zk     es有自己的分布式管理功能
格式        solr多格式      es仅json格式
搜索方案     solr传统搜索    es 新兴的实时搜索
对已有数据搜索 solr效率快        es慢
```



## es的分布式架构原理

```

核心思想: 多台机子启动多个es进程实例 ,组成一个集群.搞一个索引,这个索引可以拆分成多个shard(碎片化),每个shard存储部分数据.
每个shard有多个备份 , 也就是说只有一个可以写入primary shard ,多个replica shard同步读取(类似读写分离)

master机子宕机后 会(选举机制) 选取一个节点为master节点.
非master机子宕机后 , master会将宕机节点的primary shard身份转移到他的备份机子,恢复后 再转移回去,同步后续修改的数据

```

## es的写入原理是什么?查询原理是什么? 底层lucenne的倒排索引是什么?

### 写入流程

```
# 写入原理
 总流程: 根据hash写入 ->有个协调节点 ->路由到对应的机子接收数据写入的请求,primary写入数据后->同步数据给replica机子
   ->primary和replica 都写入成功后 协调节点返回给客户端
   
1. 数据先写入buffer,在buffer中数据是搜索不到的,同时将数据写入translog日志文件(预写日志)

2.buffer满了之后或者到一定的时间,将buffer数据refresh新到一个segment file中,这边 不是直接写入磁盘,而是先进入os cache中

  |这个过程就是refresh.
  |默认是一秒refresh一次,所以es是实时的,数据写入一秒后才能被看到
  |数据只要进入了os cache中,buffer就会被清空,数据在translog里面已经持久化磁盘去一份了
  |只要数据进入os cache,此时就可以让这个segment file的数据对外提供搜索了
  
3. 重复1,2 ,不断的清除buffer,保留translog文件 ,这样translog越来越大.到达一定的长度后,就会触发commit操作

4. 强行将os cache的数据刷入到磁盘文件中去

5. translog 也是写入os cache中去的   默认5秒写入磁盘   (机器挂了最多也就是丢失5秒的数据)

6. 将现有的translog清空,重启一个translog,默认30分钟自动执行一次commit,如果log过大也是会执行的

7.如果是删除操作,commit的时候会生成一个.del文件,里面将某个doc标识为删除状态,搜索的时候就根据.del就知道删除了

8.如果是更新操作,将原来的数据标记为删除,再新写入一条数据

9.buffer每次refersh一次,就会产生一个segment file, 定期进行合并操作,

10.每次合并的时候就会将表示为删除的doc 物理删除,然后写入磁盘 ,删除旧的segment file
```

### 查询原理

```
1.根据docId 进行hash,路由到对应的shard上面的 查询  (在shard根据负载均衡->源数据+备份数据)读取

2.搜索关键词 :全文检索 ->根据倒排索引查询
  | 因为不知道这个关键词在哪 所以发给所有的shard去查找
  | 每个shard都把匹配度高的docId 发送给协调节点
  | 然后协调节点拿到所有数据的docId 再次进行筛选最匹配的
  | 然后返回给客户端

```

### 倒排索引

```
分词 ->根据 item 再获取出数据  
比如 url A = 我爱中国 url B =我爱家乡
分词:  url A = 我 爱 中国
       url B= 我 爱 家乡
       搜索: 我  取交集 A B
       
```





## es数据量很大的情况下(数十亿级别)如何提高查询效率?

```
1.性能调优的杀手锏 -> filesyestem cache  (操作系统缓存) 
   es严重依赖底层的filesystem cache ,如果给filesyestem cache更多的内存,尽量让内存可容纳更多的索引数据文件,搜索基本就是走内存,性能会非常高
   这样就走存内存 基本上就是几百毫秒的事情了
   默认es的jvm heap是4g   比如 16g的内存  jvm 4g 剩余12g就是filesystem file
   因为: 查询数据 -> filesystem file (没有再入磁盘)->磁盘
   ps:最好情况下是 机器内存最少可以容纳你总数据量的一半

2.数据拆分 在es中尽量只保存需要搜索的数据 其他数据存入Hbase,hadoop,mysql等 这样es搜索效率就高了,其他数据再根据id 去其他位置查找

3.缓存预热 先让缓存预留出常用的搜索 避免 开始就直接磁盘查找  对于热点数据 提前访问刷入cache

4.冷热分离  冷数据放入一个index  热数据放入一个index 预热过后避免冷数据刷上去移除cache中的热 数据

5.document模型设计   尽量少执行一些复杂的操作,比如join,nested等 性能都很差
 搜索出来的复杂操作留给java去执行

6.分页查询优化 (因为是分布式的 所以分页每个节点获取的数据比最终结果数据多很多)  
  |第一套方案:利用scroll api来处理(不断通过游标获取下一页数据) 预留用户的一段时间内的数据快照 但是不能任意跳到某一页   https://blog.csdn.net/u014589856/article/details/78775233
  |第二套方案: 不允许深度分页,默认深度分页性能差
```



## es的生产集群部署架构是什么?每个索引的数据量大概有多少?每个索引大概有多少个分片?

```
部署架构: 5台机子 6核64G 集群总内存320G
集群日增张数据大概 2000w条 大概500MB ,每月6亿数据,15G  .目前运行了几个月 大概有100G左右的数据
每个索引的数据量大概有多少 : 线上有5个索引 ,每个索引数据量大概20G 
每个索引大概有多个分片  :每个索引分配8个shard,比默认的5个shard 多了3个shard

```

## 还存在部分问题

```
es底层的相关度评分算法 TF/IDF算法
deep paging
上千万数据批处理
跨机房多集群同步
搜索效果优化等等
```

