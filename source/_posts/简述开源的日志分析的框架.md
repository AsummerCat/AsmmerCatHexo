---
title: 简述开源的日志分析的框架
date: 2018-12-06 15:27:22
tags: [ELK日志分析框架]

---

# ELK日志分析框架 

Elk是elasticsearch+logstash+kibana的简称，是一个开源的日志分析的框架，在这三个组件中，这三个组件的作用如下

- elasticsearch是存储，可以理解为数据库，把数据存起来并建立索引，方便用户的查询。
- Kibana是展示用的组件，负责将数据进行可视化呈现。同时有控制台，可以直接对es进行操作，
- Logstash是这个elk的核心组件，是日志的过滤器。它负责将日志进行格式化，然后传给elasticsearch进行存储。同时logstash也有一些组件，可以进行一些数据分析的工作。

<!--more-->

#  简略架构

这三个组件之间的逻辑关系是logstash-elasticsearch-kibana，但是在实际操作中并没有那么简单。因为还有一些问题需要考虑：

- 日志怎么实时读取？
- 为了避免大并发的情况下造成的信息丢失，需要使用cache，cache怎么设置？
- 怎么实现集群化部署？集群化部署的话，负载均衡的问题怎么考虑？

因此，一个完整的elk的架构应该是这个样子的：

![ELK流程](/img/2018-12-6/ELK.png)



- Filebeat负责从web服务器上实时抓取数据，当log文件发生变化时，将文件内容吐给kafka。
- Kafka是消息队列，主要作用是在filebeat和logstash之间做缓存，避免因写入logstash的数据量过大，导致数据丢失。同样的解决方案还有redis。activemq，ribbitmq也是消息队列，但是因为filebeat没有相对应的输出格式，因此在这个方案中不做考虑。
- Zookeeper是kafka的分发系统，他负责维护整个kafka集群的负载均衡，在部署的时候，每个kafka节点上都要单独安装zookeeper，同时要保证zookeeper之间能够互相通信（2181端口）。
- Logstash是日志处理器，也是整个elk系统的核心。负责来自kafka的日志处理，然后把处理过的日志吐给elasticsearch。需要注意的是，经logstash处理过的日志都是json格式的。同时logstash还有众多插件，在这个里面需要用到的logstash-input-kafka就是插件之一。还一些有趣的函数，例如geoip，可以根据ip找出ip的所在的位置。
- Elasticsearch可以理解为是一个存储，它将logstash传来的数据存储起来、建立索引，由于elasticsearch建立的是全文索引，这点有区别于同类产品splunk，所以在查询数据的准确度和速度上都有所提高，适合于大数据检索，但是做大数据检索时，需要集群化部署，典型的用空间换时间的做法。
- Kibana是一个图形化展示的工具，能够根据elasticsearch中存储的数据进行可视化展示。同时在高版本的kibana中，还提供了开发者工具，可以直接写elastcsearch的查询语句，并进行查询，可以看作是elasticsearch的一个可视化用户端。

---

# 部署情况tips

- 不建议把elasticsearch，logstash和kibana安装到同一台机器上，因为在这三个组件中，logstash和elasticsearch是非常消耗资源的，安装在一台机器上会造成资源的相互争夺，导致效果不理想。
- 操作系统建议centos7，为了避免不必要的麻烦。java环境8，这个是logstash和elasticsearch的要求。同时，elk的版本号最好保持一致，据说低版本的kibana和elasticsearch交互时候存在bug。
- Kafka与logstash的版本问题，logstash和kafka存在一个版本对应关系，选用版本的时候，注意下版本问题，否则可能出现不能交互的错误。

---

